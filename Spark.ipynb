{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pankaj-bhandari/spark/blob/main/Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "quRqY1WA0nhT",
        "outputId": "5ab95ea9-ce5b-4304-b113-9ddc70f07b70"
      },
      "source": [
        "!pip3 install spark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.3.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        " \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"]=\"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"]=\"/content/spark-2.3.1-bin-hadoop2.7\"\n",
        " \n",
        "import findspark\n",
        "findspark.init()\n",
        " \n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.getOrCreate()\n",
        "spark\n",
        "\n",
        "num = spark.sparkContext.textFile(\"/content/sample_data/california_housing_test.csv\").count()\n",
        "print(num)\n",
        "\n",
        "num = spark.sparkContext.textFile(\"/content/sample_data/california_housing_test.csv\")\n",
        "\n",
        "num.take(10)\n",
        "\n",
        "\n",
        "docs = spark.sparkContext.textFile(\"Important.txt\").flatMap(lambda line: line.split(\" \")).map(lambda word: (word,1)).reduceByKey(lambda a,b:a +b)\n",
        "\n",
        "docs.take(10)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spark\n",
            "  Downloading spark-0.2.1.tar.gz (41 kB)\n",
            "\u001b[?25l\r\u001b[K     |████████                        | 10 kB 20.5 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20 kB 26.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 30 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 40 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 41 kB 63 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: spark\n",
            "  Building wheel for spark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spark: filename=spark-0.2.1-py3-none-any.whl size=58762 sha256=917eb3cb8567c74faf91a60ddb66daf370e0c57c57a7ef50f6e915d47d59d0fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/0e/f1/164619f9920fb447d294afaae11a7715bd442ded7225953d72\n",
            "Successfully built spark\n",
            "Installing collected packages: spark\n",
            "Successfully installed spark-0.2.1\n",
            "^C\n",
            "\n",
            "gzip: stdin: unexpected end of file\n",
            "tar: Unexpected EOF in archive\n",
            "tar: Unexpected EOF in archive\n",
            "tar: Error is not recoverable: exiting now\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 180, in _main\n",
            "    status = self.run(options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/req_command.py\", line 199, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 385, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/commands/install.py\", line 515, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/check.py\", line 103, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/operations/check.py\", line 45, in create_package_set_from_installed\n",
            "    package_set[name] = PackageDetails(dist.version, dist.requires())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3033, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3094, in parse_requirements\n",
            "    yield Requirement(line)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3101, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/packaging/requirements.py\", line 113, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1943, in parseString\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 1689, in _parseNoCache\n",
            "    tokens = self.postParse(instring, loc, tokens)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 5140, in postParse\n",
            "    retToks += ParseResults([\"\".join(tokenlist._asStringList(self.joinString))], modal=self.modalResults)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_vendor/pyparsing.py\", line 559, in __init__\n",
            "    self.__accumNames = {}\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/main.py\", line 71, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 104, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/cli/base_command.py\", line 213, in _main\n",
            "    logger.debug(\"Exception information:\", exc_info=True)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1366, in debug\n",
            "    self._log(DEBUG, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1514, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1524, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1586, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 894, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.7/logging/handlers.py\", line 71, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1127, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 1025, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 869, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pip/_internal/utils/logging.py\", line 130, in format\n",
            "    formatted = super().format(record)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 616, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "  File \"/usr/lib/python3.7/logging/__init__.py\", line 566, in formatException\n",
            "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "  File \"/usr/lib/python3.7/traceback.py\", line 104, in print_exception\n",
            "    type(value), value, tb, limit=limit).format(chain=chain):\n",
            "  File \"/usr/lib/python3.7/traceback.py\", line 508, in __init__\n",
            "    capture_locals=capture_locals)\n",
            "  File \"/usr/lib/python3.7/traceback.py\", line 363, in extract\n",
            "    f.line\n",
            "  File \"/usr/lib/python3.7/traceback.py\", line 285, in line\n",
            "    self._line = linecache.getline(self.filename, self.lineno).strip()\n",
            "  File \"/usr/lib/python3.7/linecache.py\", line 16, in getline\n",
            "    lines = getlines(filename, module_globals)\n",
            "  File \"/usr/lib/python3.7/linecache.py\", line 47, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "  File \"/usr/lib/python3.7/linecache.py\", line 136, in updatecache\n",
            "    with tokenize.open(fullname) as fp:\n",
            "  File \"/usr/lib/python3.7/tokenize.py\", line 449, in open\n",
            "    encoding, lines = detect_encoding(buffer.readline)\n",
            "  File \"/usr/lib/python3.7/tokenize.py\", line 418, in detect_encoding\n",
            "    first = read_or_stop()\n",
            "  File \"/usr/lib/python3.7/tokenize.py\", line 376, in read_or_stop\n",
            "    return readline()\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-428f3fa859a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SPARK_HOME\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/spark-2.3.1-bin-hadoop2.7\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'findspark'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSxbu4Ri06LO"
      },
      "source": [
        "# import libraries\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql import functions as f\n",
        "from pyspark.sql.types import *\n",
        "import datetime\n",
        "from datetime import timedelta, date\n",
        "import time\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SQLContext, HiveContext\n",
        "from pyspark.sql import DataFrameStatFunctions as statFunc\n",
        "from pyspark.sql.functions import regexp_extract, col\n",
        "\n",
        "#spark settings\n",
        "conf = SparkConf().setAppName('Smapi_myvf_es_pyspark')\n",
        "#conf.set(\"spark.submit.deployMode\",\"cluster\")\n",
        "conf.set(\"spark.dynamicAllocation.enabled\", \"true\")\n",
        "conf.set(\"spark.shuffle.service.enabled\", \"true\")\n",
        "conf.set(\"spark.dynamicAllocation.executorIdleTimeout\", \"2m\")\n",
        "conf.set(\"spark.dynamicAllocation.minExecutors\", \"1\")\n",
        "conf.set(\"spark.dynamicAllocation.maxExecutors\", \"1300\")\n",
        "conf.set(\"spark.rpc.io.serverThreads\", \"64\")\n",
        "conf.set(\"spark.memory.offHeap.enabled\", \"true\")\n",
        "conf.set(\"spark.memory.offHeap.size\", \"3g\")\n",
        "conf.set(\"spark.executor.memory\", \"3g\")\n",
        "conf.set(\"spark.yarn.executor.memoryOverhead\", \"3600\")\n",
        "conf.set(\"spark.shuffle.file.buffer\",\"1MB\")\n",
        "conf.set(\"spark.unsafe.sorter.spill.reader.buffer.size\",\"1MB\")\n",
        "conf.set(\"spark.file.transferTo\",\"false\")\n",
        "conf.set(\"spark.io.compression.lz4.blockSize\",\"512KB\")\n",
        "conf.set(\"spark.shuffle.service.index.cache.entries\",\"2048\")\n",
        "#conf.set(\"spark.serializer\",\"org.apache.spark.serializer.KryoSerializer\")\n",
        "#conf.set(\"spark.kryoserializer.buffer\",\"1MB\")\n",
        "#conf.set(\"spark.default.parallelism\",\"26\")\n",
        "#conf.set(\"spark.sql.shuffle.partitions\",26)\n",
        "#conf.set(\"spark.dynamicAllocation.initialExecutors\",\"1\")\n",
        "#conf.set(\"spark.yarn.historyServer.address\",\"10.109.214.142.18089\")\n",
        "#conf.set(\"spark.yarn.historyServer.allowTracking\",\"true\")\n",
        "\n",
        "sc = SparkContext(conf=conf)\n",
        "sqlContext=SQLContext(sc)\n",
        "\n",
        "##### Data import #####\n",
        "#yesterday_time=date.today() - timedelta(1) #put 1 here\n",
        "#yesterday_date=yesterday_time.strftime(\"%Y%m%d\")\n",
        "#today_date=date.today().strftime(\"%Y%m%d\")\n",
        "for yesterday_date in 20190701,20190702,20190703,20190704,20190705:\n",
        "\t#read local json files\n",
        "\tfield = [StructField('x_vf_net_type', StringType(), True),StructField('x_vf_screen_rotation', StringType(), True),\n",
        "\t\t\t\tStructField('x_vf_trace_source', StringType(), True),StructField('x_vf_trace_subject_region', StringType(), True),\n",
        "\t\t\t\tStructField('x_vf_trace_subject_id', StringType(), True),\n",
        "\t\t\t\tStructField('event_description', StringType(), True),\n",
        "\t\t\t\tStructField('x_vf_trace_session_id', StringType(), True),StructField('x_vf_trace_install_id', StringType(), True),\n",
        "\t\t\t\tStructField('x_vf_trace_source_version', StringType(), True),StructField('x_vf_trace_timestamp', StringType(), True),\n",
        "\t\t\t\tStructField('x_vf_trace_network_bearer', StringType(), True),StructField('x_vf_trace_user_agent', StringType(), True),\n",
        "\t\t\t\tStructField('x_vf_net_band', StringType(), True),StructField('x_vf_event_page', StringType(), True),\n",
        "\t\t\t\tStructField('x_vf_event_subpage', StringType(), True),StructField('x_vf_trace_platform', StringType(), True),\n",
        "\t\t\t\tStructField('x_vf_trace_os_version', StringType(), True),StructField('x_vf_trace_os_name', StringType(), True),\n",
        "\t\t\t\tStructField('x_vf_trace_application_name', StringType(), True),StructField('x_vf_trace_mcc', StringType(), True),\n",
        "\t\t\t\tStructField('x_vf_trace_mnc', StringType(), True),StructField('x_vf_trace_locale', StringType(), True),\n",
        "\t\t\t\tStructField('x_vf_trace_accept_language', StringType(), True),StructField('x_vf_event_type', StringType(), True),\n",
        "\t\t\t\tStructField('x_vf_event_element', StringType(), True),StructField('x_vf_trace_seclib_version', StringType(), True),\n",
        "\t\t\t\tStructField('sec_received_at', StringType(), True),StructField('x_vf_trace_host', StringType(), True),\n",
        "\t\t\t\tStructField('x_vf_trace_origin', StringType(), True),StructField('x_vf_screen_width', StringType(), True),\n",
        "\t\t\t\tStructField('x_vf_screen_height', StringType(), True),StructField('x_vf_trace_transaction_id', StringType(), True),\n",
        "\t\t\t\tStructField('event_description_dns', StringType(), True), StructField('event_description_duration', StringType(), True),\n",
        "\t\t\t\tStructField('event_description_httpCode', StringType(), True), StructField('event_description_redirect', StringType(), True),\n",
        "\t\t\t\tStructField('event_description_request', StringType(), True), StructField('event_description_response', StringType(), True),\n",
        "\t\t\t\tStructField('event_description_ssl', StringType(), True), StructField('event_description_start', StringType(), True),\n",
        "\t\t\t\tStructField('event_description_tcp', StringType(), True), StructField('event_description_type', StringType(), True),\n",
        "\t\t\t\tStructField('event_description_url', StringType(), True), StructField('host', StringType(), True),\n",
        "\t\t\t\tStructField('origin', StringType(), True), StructField('refer', StringType(), True), StructField('topic', StringType(), True)]\n",
        "\tschema = StructType(field)\n",
        "\t#in_dir = \"/services/myvf_gr/in/\"+yesterday_date+\"/myvf-DE-js*\"\n",
        "\tdf = sqlContext.read.json(\"/services/myvf_GR/in/\"+str(yesterday_date)+\"/myvf-GR-js*\",schema=schema)\n",
        "\t\n",
        "\n",
        "\t#standardization\n",
        "\tdf = df.withColumnRenamed(\"x-vf-trace-height\",\"x_vf_trace_height\")\\\n",
        "\t\t\t.withColumnRenamed(\"x-vf-trace-width\",\"x_vf_trace_width\")\n",
        "\n",
        "\t#Data analysis\n",
        "\tdf = df.withColumn(\"sec_received_at\",df.sec_received_at/1000)\n",
        "\tdf = df.withColumn(\"x_vf_trace_timestamp\",df.x_vf_trace_timestamp/1000)\n",
        "\tdf = df.withColumn(\"sec_received_at\", df[\"sec_received_at\"].cast(IntegerType()))\n",
        "\tdf = df.withColumn(\"x_vf_trace_timestamp\", df[\"x_vf_trace_timestamp\"].cast(IntegerType()))\n",
        "\n",
        "\t##Extracing the page_load\n",
        "\tdf_filt1 = df.withColumn(\"page_load\",f.when(df.x_vf_event_element.like(\"%page-loaded%\") |\\\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tdf.x_vf_event_element.like(\"%Load Time%\"),f.regexp_extract('event_description', r\"([0-9]+\\.[0-9]+)\",1)))\n",
        "\tdf_filt1 = df_filt1.withColumn(\"page_load\", df_filt1[\"page_load\"].cast(FloatType()))\n",
        "\n",
        "\t##Extracing the response_time\n",
        "\tdf_filt1 = df_filt1.withColumnRenamed(\"event_description_duration\",\"response_time\")\n",
        "\n",
        "\t##Extracing the response_code\n",
        "\tdf_filt1 = df_filt1.withColumn(\"response_code\",f.col(\"event_description_httpCode\"))\n",
        "\n",
        "\t##Extracing the urls\n",
        "\tdf_filt1 = df_filt1.withColumn(\"urls\",f.regexp_extract('event_description_url', r\"((http|https|sftp|ftp|file).{3})([\\.a-zA-Z0-9\\?\\-\\_\\&\\+\\%\\=]+)\",3))\n",
        "\tdf_filt1 = df_filt1.drop(\"event_description_url\")\n",
        "\n",
        "\t##Extracing the browser\n",
        "\tdf_filt1 = df_filt1.withColumn(\"browser\",f.when(df_filt1.x_vf_trace_user_agent.like(\"%MSIE%\") | df_filt1.x_vf_trace_user_agent.like(\"%Trident%\"),\"IE\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t  .when(df_filt1.x_vf_trace_user_agent.like(\"%Mozilla%\") & (df_filt1.x_vf_trace_user_agent.like(\"%iPad%\") |\\\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdf_filt1.x_vf_trace_user_agent.like(\"%iPhone%\") |\\\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdf_filt1.x_vf_trace_user_agent.like(\"%Mac Os X%\")),\"Safari\")        \n",
        "\t\t\t\t\t\t\t\t\t\t\t  .when(df_filt1.x_vf_trace_user_agent.like(\"%Chrome%\") & df_filt1.x_vf_trace_user_agent.like(\"%Mobile Safari%\"),\"Chrome Mobile\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t  .when(df_filt1.x_vf_trace_user_agent.like(\"%Chrome%\") & df_filt1.x_vf_trace_user_agent.like(\"%Safari%\"),\"Chrome\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t  .when(f.col(\"x_vf_trace_user_agent\").isNull() | (f.col(\"x_vf_trace_user_agent\") ==''),\"\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t  .otherwise('Mozilla'))\n",
        "\n",
        "\t##Extracing the device_type\n",
        "\tdf_filt1 = df_filt1.withColumn(\"device_type\",f.when(f.lower(df_filt1.x_vf_trace_user_agent).like(\"%ipad%\") |\\\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tf.lower(df_filt1.x_vf_trace_user_agent).like(\"%tablet%\"),\"Tablet\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t  .when(f.lower(df_filt1.x_vf_trace_user_agent).like(\"%mobile%\") |\\\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tf.lower(df_filt1.x_vf_trace_user_agent).like(\"%iphone%\") |\\\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tf.lower(df_filt1.x_vf_trace_user_agent).like(\"%android%\") & ~f.lower(df_filt1.x_vf_trace_user_agent).like(\"%tablet%\"),\"Mobile\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t  .when(f.col(\"x_vf_trace_user_agent\").isNull() | (f.col(\"x_vf_trace_user_agent\") ==''),\"\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t  .otherwise('Desktop'))\n",
        "\n",
        "\t##Extracing the platform\n",
        "\tdf_filt1 = df_filt1.withColumn(\"platform\",f.when(f.lower(df_filt1.x_vf_trace_platform).like(\"%android%\"),\"Android\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_platform).like(\"%ios%\"),\"iOS\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_platform).like(\"%js%\"),\"Web\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t.otherwise('N/A'))\n",
        "\t### extract os version ###\n",
        "\tdf_filt1 = df_filt1.withColumn(\"os_version\",f.concat(df_filt1.x_vf_trace_os_name, df_filt1.x_vf_trace_os_version))\n",
        "\n",
        "\t### change os name format ###\n",
        "\tdf_filt1 = df_filt1.withColumn(\"os_name\",f.when(df_filt1.x_vf_trace_os_name.like(\"M\"),\"Marshmallow (Android)\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t.when(df_filt1.x_vf_trace_os_name.like(\"N\"),\"Nougat (Android)\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t.when(df_filt1.x_vf_trace_os_name.like(\"Windows\"),\"Windows\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t.when(df_filt1.x_vf_trace_os_name.like(\"Linux\"),\"Linux\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t.when(df_filt1.x_vf_trace_os_name.like(\"N_MR1\"),\"Nougat_MR1 (Android)\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t.when(df_filt1.x_vf_trace_os_name.like(\"KITKAT\"),\"KITKAT (Android)\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t.when(df_filt1.x_vf_trace_os_name.like(\"L\"),\"Lollipop (Android)\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t.when(df_filt1.x_vf_trace_os_name.like(\"Mac OS X\"),\"Mac OS X\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t.when(df_filt1.x_vf_trace_os_name.like(\"iOS\"),\"iOS\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t.when(df_filt1.x_vf_trace_os_name.like(\"Android\"),\"Android\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t.when(df_filt1.x_vf_trace_os_name.like(\"O_MR1\"),\"Oreo_MR1 (Android)\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t.when(df_filt1.x_vf_trace_os_name.like(\"P\"),\"Pie (Android)\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t.when(df_filt1.x_vf_trace_os_name.like(\"O\"),\"Oreo (Android)\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t.when(df_filt1.x_vf_trace_os_name.like(\"LOLLIPOP_MR1\"),\"Lollipop_MR1 (Android)\"))\n",
        "\n",
        "\t### change network bearer format ###\n",
        "\tdf_filt1 = df_filt1.withColumn(\"network_bearer\",f.when(f.lower(df_filt1.x_vf_trace_network_bearer).like(\"2g\"),\"2G\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_network_bearer).like(\"3g\"),\"3G\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_network_bearer).like(\"4g\"),\"4G\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_network_bearer).like(\"lte\"),\"LTE\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_network_bearer).like(\"bluetooth tethering\"),\"Bluetooth Tethering\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_network_bearer).like(\"ethernet\"),\"Ethernet\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_network_bearer).like(\"mobile\"),\"Mobile\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_network_bearer).like(\"slow-2g\"),\"slow-2g\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_network_bearer).like(\"vpn\"),\"VPN\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_network_bearer).like(\"wifi\"),\"WiFi\"))\n",
        "\n",
        "\t##Extracing the satisfaction_level\n",
        "\tdf_filt1 = df_filt1.withColumn(\"satisfaction_level\",f.when(df_filt1.page_load<3,\"Satisfied\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t .when((df_filt1.page_load>=3) & (df_filt1.page_load<12),\"Tolerated\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t .when(df_filt1.page_load>=12,\"Frustrated\"))\n",
        "\n",
        "\t### extract subject region ###\n",
        "\tdf_filt1 = df_filt1.withColumn(\"subject_region\",f.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%al%\"),\"Albania\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%ar%\"),\"Argentina\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%at%\"),\"Austria\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%be%\"),\"Belgium\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%br%\"),\"Brazil\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%ca%\"),\"Canada\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%ch%\"),\"Switzerland\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%co%\"),\"Colombia\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%cy%\"),\"Cyprus\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%cz%\"),\"Czech\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%de%\"),\"Germany\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%dk%\"),\"Denmark\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%ee%\"),\"Estonia\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%eg%\"),\"Egypt\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%es%\"),\"Spain\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%fi%\"),\"Finland\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%fr%\"),\"France\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%gb%\"),\"Great Britain\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%gr%\"),\"Greece\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%hk%\"),\"Hong Kong\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%hr%\"),\"Croatia\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%hu%\"),\"Hungary\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%id%\"),\"Indonesia\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%ie%\"),\"Ireland\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%il%\"),\"Israel\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%in%\"),\"India\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%iq%\"),\"Iraq\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%ir%\"),\"Iran\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%is%\"),\"Iceland\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%it%\"),\"Italy\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%jp%\"),\"Japan\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%kp%\"),\"North Korea\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%kr%\"),\"South Korea\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%lu%\"),\"Luxembourg\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%mt%\"),\"Malta\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%mx%\"),\"Mexico\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%nl%\"),\"Netherlands\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%no%\"),\"Norway\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%np%\"),\"Nepal\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%nz%\"),\"New Zealand\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%pa%\"),\"Panama\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%pe%\"),\"Peru\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%ph%\"),\"Philippines\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%pk%\"),\"Pakistan\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%pl%\"),\"Poland\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%pt%\"),\"Portugal\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%py%\"),\"Paraguay\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%qa%\"),\"Qatar\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%ro%\"),\"Romania\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%ru%\"),\"Russia\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%sa%\"),\"Saudi Arabia\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%se%\"),\"Sweden\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%sg%\"),\"Singapore\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%si%\"),\"Slovenia\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%sk%\"),\"Slovakia\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%sm%\"),\"San Marino\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%th%\"),\"Thailand\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%tn%\"),\"Tunisia\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%tr%\"),\"Turkey\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%tw%\"),\"Taiwan\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%ua%\"),\"Ukraine\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%us%\"),\"United States\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.when(f.lower(df_filt1.x_vf_trace_subject_region).like(\"%za%\"),\"South Africa\")\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t.otherwise('Others'))\n",
        "\n",
        "\t##Extracing the tti\n",
        "\tdf_filt1 = df_filt1.withColumn(\"tti\",f.when(df_filt1.x_vf_event_element.like(\"%page-dom-loaded%\"),f.regexp_extract('event_description', r\"([0-9]+\\.[0-9]+)\",1))\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t.otherwise(''))\n",
        "\n",
        "\t##Extracing the Tableau calculated field Failed_Requests                                     \n",
        "\tdf_filt1 = df_filt1.withColumn(\"Failed_Requests\",f.when((df_filt1[\"response_code\"].cast(FloatType()) > 403) |\\\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(df_filt1[\"response_code\"].cast(FloatType()) == 400),1).otherwise(0))\n",
        "\n",
        "\tdf_filt1 = df_filt1.drop(\"dt\")\n",
        "\n",
        "\t##Extracing the session_duration\n",
        "\tdf_min_max_copy = df_filt1.groupby(['platform','x_vf_trace_session_id']).agg(f.max('x_vf_trace_timestamp')\\\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t.alias('max_session_timestamp'),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t f.min('x_vf_trace_timestamp')\\\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t.alias('min_session_timestamp'))\n",
        "\tdf_min_max_copy = df_min_max_copy.withColumn('session_duration',\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t (df_min_max_copy.max_session_timestamp-df_min_max_copy.min_session_timestamp))\n",
        "\tdf_min_max_copy = df_min_max_copy.drop(\"max_session_timestamp\")\\\n",
        "\t\t\t\t\t\t\t\t\t\t.drop(\"min_session_timestamp\")\n",
        "\n",
        "\t##Joining two DF (df_filt1,df_min_max_copy)\n",
        "\tdf_filt1 = df_filt1.join(df_min_max_copy,['platform','x_vf_trace_session_id'],how='left')\n",
        "\n",
        "\t#memory help\n",
        "\tdf_filt1.cache()\n",
        "\n",
        "\t#specify column types\n",
        "\tdf_final = df_filt1.withColumn(\"tti\", df_filt1[\"tti\"].cast(FloatType()))\n",
        "\t#df_final = df_final.withColumn(\"page_load\", df_final[\"page_load\"].cast(FloatType()))\n",
        "\tdf_final = df_final.withColumn(\"response_time\", df_final[\"response_time\"].cast(FloatType()))\n",
        "\tdf_final = df_final.withColumn(\"session_duration\", df_final[\"session_duration\"].cast(FloatType()))\n",
        "\n",
        "\t##Logic to give_percentile\n",
        "\tdef give_percentile(plat,metrics,perc):\n",
        "\t\tdf_perc = df_final.filter(df_final.platform.like('%' + plat + '%'))\n",
        "\t\tdf_perc = df_perc.filter(df_final[metrics]>0)\n",
        "\t\tdf_perc.coalesce(1)\n",
        "\t\tpercentile_val = df_perc.approxQuantile(metrics, [perc], 0.05)\n",
        "\t\tif len(percentile_val)>0:\n",
        "\t\t\tpercentile_val = float(percentile_val[0])\n",
        "\t\telse:\n",
        "\t\t\tpercentile_val = float(0)\n",
        "\t\treturn round(percentile_val,5)\n",
        "\n",
        "\t##Creating DF to calling above functions\n",
        "\tdf_agg = sqlContext.createDataFrame([Row(platform='Web',\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tpercentile_tti_50=give_percentile(\"Web\",\"tti\",0.5),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tpercentile_tti_80=give_percentile(\"Web\",\"tti\",0.8),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tpercentile_tti_90=give_percentile(\"Web\",\"tti\",0.9),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tpercentile_tti_95=give_percentile(\"Web\",\"tti\",0.95),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tpercentile_page_load_50=give_percentile(\"Web\",\"page_load\",0.5),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tpercentile_page_load_80=give_percentile(\"Web\",\"page_load\",0.8),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tpercentile_page_load_90=give_percentile(\"Web\",\"page_load\",0.9),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tpercentile_page_load_95=give_percentile(\"Web\",\"page_load\",0.95),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tpercentile_time_diff_50=give_percentile(\"Web\",\"session_duration\",0.5),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tpercentile_time_diff_80=give_percentile(\"Web\",\"session_duration\",0.8),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tpercentile_time_diff_90=give_percentile(\"Web\",\"session_duration\",0.9),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tpercentile_time_diff_95=give_percentile(\"Web\",\"session_duration\",0.95),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tpercentile_response_time_50=give_percentile(\"Web\",\"response_time\",0.5),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tpercentile_response_time_80=give_percentile(\"Web\",\"response_time\",0.8),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tpercentile_response_time_90=give_percentile(\"Web\",\"response_time\",0.9),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tpercentile_response_time_95=give_percentile(\"Web\",\"response_time\",0.95))])\n",
        "\n",
        "\t##Taking date in format YYYY-MM-DD\n",
        "\tdf_final = df_final.withColumn(\"sec_received_at\",f.from_unixtime(df.sec_received_at,'yyyy-MM-dd')).cache() \n",
        "\n",
        "\t##Extracing the isBounced\n",
        "\tdf_max_dur_session = df_final.groupby(['x_vf_trace_session_id']).agg({'session_duration': 'max'})\n",
        "\tdf_max_dur_session = df_max_dur_session.select(f.col('x_vf_trace_session_id'),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tf.col(\"max(session_duration)\").alias(\"session_duration\"))\n",
        "\tdf_max_dur_session = df_max_dur_session.withColumn(\"isBounced\",f.when(df_max_dur_session.session_duration < 10,1)) #.otherwise(0)\n",
        "\tdf_max_dur_session = df_max_dur_session.drop(\"session_duration\")\n",
        "\n",
        "\t#compute percentiles by x_vf_event_page\n",
        "\tpercentiles=['0.5','0.8','0.9','0.95']\n",
        "\tlist_df_p_event_page = []\n",
        "\n",
        "\tfor p in percentiles:\n",
        "\t\tmagic_percentile_tti = f.expr('percentile_approx(tti,'+ p +')')\n",
        "\t\tmagic_percentile_resp_time = f.expr('percentile_approx(response_time,'+ p +')')\n",
        "\t\tdf_p_event_page = df_final.groupBy(['platform','x_vf_event_page'])\\\n",
        "\t\t\t\t\t\t\t\t  .agg(magic_percentile_tti.alias('percentile_tti_by_event_page_'+ str(int(float(p)*100))),\n",
        "\t\t\t\t\t\t\t\t\t   magic_percentile_resp_time.alias('percentile_response_time_by_event_page_'+ str(int(float(p)*100))))\n",
        "\t\tlist_df_p_event_page.append(df_p_event_page)\n",
        "\t\t\t\n",
        "\tdf_p_event_page = list_df_p_event_page[0]\n",
        "\n",
        "\tfor df_next in list_df_p_event_page[1:]:\n",
        "\t\tdf_p_event_page = df_p_event_page.join(df_next,['platform','x_vf_event_page'],how='inner')     \n",
        "\n",
        "\t#compute percentiles by url\n",
        "\tlist_df_p_urls = []\n",
        "\n",
        "\tlist_field_p = [[StructField('platform', StringType(), True),StructField('urls', StringType(), True),\n",
        "\t\t\t\t\t StructField('percentile_tti_by_url_50', DoubleType(), True),\n",
        "\t\t\t\t\t StructField('percentile_response_time_by_url_50', DoubleType(), True)],\n",
        "\t\t\t\t\t[StructField('platform', StringType(), True),StructField('urls', StringType(), True),\n",
        "\t\t\t\t\t StructField('percentile_tti_by_url_80', DoubleType(), True),\n",
        "\t\t\t\t\t StructField('percentile_response_time_by_url_80', DoubleType(), True)],\n",
        "\t\t\t\t\t[StructField('platform', StringType(), True),StructField('urls', StringType(), True),\n",
        "\t\t\t\t\t StructField('percentile_tti_by_url_90', DoubleType(), True),\n",
        "\t\t\t\t\t StructField('percentile_response_time_by_url_90', DoubleType(), True)],\n",
        "\t\t\t\t\t[StructField('platform', StringType(), True),StructField('urls', StringType(), True),\n",
        "\t\t\t\t\t StructField('percentile_tti_by_url_95', DoubleType(), True),\n",
        "\t\t\t\t\t StructField('percentile_response_time_by_url_95', DoubleType(), True)]]\n",
        "\n",
        "\tfor p in percentiles:\n",
        "\t\tmagic_percentile_tti = f.expr('percentile_approx(tti,'+ p +')')\n",
        "\t\tmagic_percentile_resp_time = f.expr('percentile_approx(response_time,'+ p +')')\n",
        "\t\tdf_p_urls = df_final.groupBy(['platform','urls']).agg(magic_percentile_tti.alias('percentile_tti_by_url_'+ str(int(float(p)*100))),\\\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  magic_percentile_resp_time.alias('percentile_response_time_by_url_'+ str(int(float(p)*100))))\n",
        "\t\tlist_df_p_urls.append(df_p_urls)\n",
        "\t\t\n",
        "\tlist_df_p_urls_r = []\n",
        "\n",
        "\tcnt=0\n",
        "\tfor df_t in list_df_p_urls:\n",
        "\t\tp_dir = '/services/dproduct/smapi_analytics/myvf_gr/out/cockpit/temp/tmp/gr/web/'+str(yesterday_date)+'/df_p_cross'+ str(cnt)\n",
        "\t\tdf_t.write.format('json').mode(\"overwrite\").save(p_dir) \n",
        "\t\tcnt+=1\n",
        "\t\t\n",
        "\tfor cnt in range(len(list_df_p_urls)):\n",
        "\t\tp_dir = '/services/dproduct/smapi_analytics/myvf_gr/out/cockpit/temp/tmp/gr/web/'+str(yesterday_date)+'/df_p_cross'+ str(cnt)\n",
        "\t\tdf_r = sqlContext.read.json(p_dir,schema=StructType(list_field_p[cnt]))\n",
        "\t\tlist_df_p_urls_r.append(df_r)\n",
        "\t\t\n",
        "\tdf_p_urls = list_df_p_urls_r[0]\n",
        "\tfor df_next in list_df_p_urls_r[1:]:\n",
        "\t\tdf_p_urls = df_p_urls.join(df_next,['platform','urls'],how='inner')\n",
        "\n",
        "\t##Group by and same occurences count\n",
        "\tdf_final_main =df_final.groupby([\"platform\"\n",
        "\t,\"x_vf_trace_session_id\"\n",
        "\t,\"x_vf_trace_subject_id\"\n",
        "\t,\"subject_region\"\n",
        "\t,\"x_vf_trace_source_version\"\n",
        "\t,\"sec_received_at\"\n",
        "\t,\"network_bearer\"\n",
        "\t,\"x_vf_event_page\"\n",
        "\t,\"os_name\"\n",
        "\t,\"x_vf_event_type\"\n",
        "\t,\"x_vf_event_element\"\n",
        "\t,\"x_vf_trace_seclib_version\"\n",
        "\t,\"urls\"\n",
        "\t,\"browser\"\n",
        "\t,\"device_type\"\n",
        "\t,\"satisfaction_level\"\n",
        "\t,\"response_code\"]).agg(\n",
        "\tf.count(\"response_time\").cast(IntegerType()).alias(\"response_time_count\"),\n",
        "\tf.sum(\"response_time\").cast(FloatType()).alias(\"response_time_sum\"),\n",
        "\tf.count(\"tti\").cast(IntegerType()).alias(\"tti_count\"),\n",
        "\tf.sum(\"tti\").cast(FloatType()).alias(\"tti_sum\"),\n",
        "\tf.count(\"page_load\").cast(IntegerType()).alias(\"page_load_count\"),\n",
        "\tf.sum(\"page_load\").cast(FloatType()).alias(\"page_load_sum\"),\n",
        "\tf.count(\"session_duration\").cast(IntegerType()).alias(\"session_duration_count\"),\n",
        "\tf.sum(\"session_duration\").cast(FloatType()).alias(\"session_duration_sum\"),\n",
        "\tf.sum(\"Failed_Requests\").cast(FloatType()).alias(\"Failed_Requests_sum\"),\n",
        "\tf.count(\"platform\").cast(IntegerType()).alias(\"Total_non_agg_records\") \n",
        "\t)\n",
        "\t\n",
        "\n",
        "\tlist_f = [StructField(\"platform\",StringType(),True),\n",
        "\t\t\t  StructField(\"x_vf_trace_session_id\",StringType(),True),\n",
        "\t\t\t  StructField(\"x_vf_trace_subject_id\",StringType(),True),\n",
        "\t\t\t  StructField(\"subject_region\",StringType(),False),\n",
        "\t\t\t  StructField(\"x_vf_trace_source_version\",StringType(),True),\n",
        "\t\t\t  StructField(\"sec_received_at\",StringType(),True),\n",
        "\t\t\t  StructField(\"network_bearer\",StringType(),True),\n",
        "\t\t\t  StructField(\"x_vf_event_page\",StringType(),True),\n",
        "\t\t\t  StructField(\"os_name\",StringType(),True),\n",
        "\t\t\t  StructField(\"x_vf_event_type\",StringType(),True),\n",
        "\t\t\t  StructField(\"x_vf_event_element\",StringType(),True),\n",
        "\t\t\t  StructField(\"x_vf_trace_seclib_version\",StringType(),True),\n",
        "\t\t\t  StructField(\"urls\",StringType(),True),\n",
        "\t\t\t  StructField(\"browser\",StringType(),False),\n",
        "\t\t\t  StructField(\"device_type\",StringType(),False),\n",
        "\t\t\t  StructField(\"satisfaction_level\",StringType(),False),\n",
        "\t\t\t  StructField(\"response_code\",StringType(),True),\n",
        "\t\t\t  StructField(\"session_duration_count\",IntegerType(),False),\n",
        "\t\t\t  StructField(\"session_duration_sum\",FloatType(),True),\n",
        "\t\t\t  StructField(\"Failed_Requests_sum\",FloatType(),True)]\n",
        "\n",
        "\t## write data to disk before join in order to make the job stable\n",
        "\tlist_event_page_p =[StructField('x_vf_event_page',StringType(),True),\n",
        "\t\t\t\t\t\tStructField('platform',StringType(),False),\n",
        "\t\t\t\t\t\tStructField('percentile_tti_by_event_page_50',FloatType(),True),\n",
        "\t\t\t\t\t\tStructField('percentile_response_time_by_event_page_50',FloatType(),True),\n",
        "\t\t\t\t\t\tStructField('percentile_tti_by_event_page_80',FloatType(),True),\n",
        "\t\t\t\t\t\tStructField('percentile_response_time_by_event_page_80',FloatType(),True),\n",
        "\t\t\t\t\t\tStructField('percentile_tti_by_event_page_90',FloatType(),True),\n",
        "\t\t\t\t\t\tStructField('percentile_response_time_by_event_page_90',FloatType(),True),\n",
        "\t\t\t\t\t\tStructField('percentile_tti_by_event_page_95',FloatType(),True),\n",
        "\t\t\t\t\t\tStructField('percentile_response_time_by_event_page_95',FloatType(),True)]\n",
        "\n",
        "\tevent_page_dir = '/services/dproduct/smapi_analytics/myvf_gr/out/cockpit/temp/tmp/gr/web/'+str(yesterday_date)+'/df_p_event_page'\n",
        "\tdf_p_event_page.write.format('json').mode(\"overwrite\").save(event_page_dir)\n",
        "\tdf_p_event_page = sqlContext.read.json(event_page_dir ,schema=StructType(list_event_page_p))\n",
        "\n",
        "\t## join\n",
        "\tdf_final_main = df_final_main.join(df_max_dur_session,['x_vf_trace_session_id'],how='left')\n",
        "\tdf_final_main = df_final_main.join(df_agg,['platform'],how='left')\n",
        "\tdf_final_main = df_final_main.join(df_p_event_page,['platform','x_vf_event_page'],how='left') \n",
        "\tdf_final_main = df_final_main.join(df_p_urls,['platform','urls'],how='left')\n",
        "\n",
        "\t##### Export Data #####\n",
        "\tdf_final_main.write.mode(\"overwrite\").option(\"maxRecordsPerFile\", 1000000).parquet(\"/services/dproduct/smapi_analytics/myvf_gr/out/cockpit/web/dt=\"+str(yesterday_date))\n",
        "\t#out_dir = \"/services/myvf_gr/out/web/dt=\"+yesterday_date\n",
        "\t#df_final_main.repartition(30).write.format(\"parquet\").mode(\"overwrite\").save(\"/services/dproduct/smapi_analytics/myvf_gr/out/cockpit/web/dt=\"+str(yesterday_date))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Rh3-Vt9Nev9"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "### Working with Notebooks in Colab\n",
        "- [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "- <img src=\"/img/new.png\" height=\"20px\" align=\"left\" hspace=\"4px\" alt=\"New\"></img>\n",
        " [TensorFlow 2 in Colab](/notebooks/tensorflow_version.ipynb)\n",
        "\n",
        "<a name=\"working-with-data\"></a>\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb) \n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas](/notebooks/mlcc/intro_to_pandas.ipynb)\n",
        "- [Tensorflow concepts](/notebooks/mlcc/tensorflow_programming_concepts.ipynb)\n",
        "\n",
        "<a name=\"using-accelerated-hardware\"></a>\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TensorFlow with TPUs](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-H6Lw1vyNNd"
      },
      "source": [
        "<a name=\"machine-learning-examples\"></a>\n",
        "\n",
        "## Machine Learning Examples\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colaboratory makes possible, check out these  tutorials using models from [TensorFlow Hub](https://tfhub.dev).\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Retraining an Image Classifier](https://tensorflow.org/hub/tutorials/tf2_image_retraining): Build a Keras model on top of a pre-trained image classifier to distinguish flowers.\n",
        "- [Text Classification](https://tensorflow.org/hub/tutorials/tf2_text_classification): Classify IMDB movie reviews as either *positive* or *negative*.\n",
        "- [Style Transfer](https://tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization): Use deep learning to transfer style between images.\n",
        "- [Multilingual Universal Sentence Encoder Q&A](https://tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa): Use a machine learning model to answer questions from the SQuAD dataset.\n",
        "- [Video Interpolation](https://tensorflow.org/hub/tutorials/tweening_conv3d): Predict what happened in a video between the first and the last frame.\n"
      ]
    }
  ]
}