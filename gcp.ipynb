{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pankaj-bhandari/spark/blob/main/gcp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aM-SQrYfv9M"
      },
      "source": [
        "# STEP 1: Libraries needed\n",
        "from datetime import timedelta, date\n",
        "import time\n",
        "import datetime\n",
        "from airflow import DAG\n",
        "from airflow.contrib.operators import bigquery_operator\n",
        "from airflow.contrib.operators.dataproc_operator import DataprocClusterCreateOperator, DataProcPySparkOperator, \\\n",
        "    DataprocClusterDeleteOperator\n",
        "from airflow.contrib.operators.gcs_to_bq import GoogleCloudStorageToBigQueryOperator\n",
        "from airflow.contrib.operators.gcs_delete_operator import GoogleCloudStorageDeleteOperator\n",
        "from airflow.operators import BashOperator, PythonOperator\n",
        "from airflow.models import Variable\n",
        "from airflow.utils.trigger_rule import TriggerRule\n",
        "\n",
        " \n",
        "\n",
        "# STEP 2:Define a start date\n",
        "# In this case yesterday\n",
        "YESTERDAY = datetime.datetime.now() - datetime.timedelta(days=1)\n",
        "# These are stored as a Variables in our Airflow Environment.\n",
        "BUCKET = Variable.get('gcs_bucket')  # GCS bucket with our data.\n",
        "\n",
        " \n",
        "\n",
        "##Variables\n",
        "# 1.Spark file path\n",
        "spark_job_esim_thales_inventory = 'gs://' + BUCKET + '/spark_code/eSIM_thales_inventory.py'\n",
        "\n",
        " \n",
        "\n",
        "# 2.Temp GCS path where Spark code put output\n",
        "# delete_esim_thales_inventory = 'gs://' + BUCKET + '/temp_out/inventory/*'\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "# 3.BQ table Name\n",
        "output_table_esim_thales_inventory = 'vf-grp-cpsa-prd-cpsoi-10.esim.thales_inventory'\n",
        "\n",
        " \n",
        "\n",
        "# 4. Date Import Logic to run automation/daily basis\n",
        "#start_date = date(2021, 5, 29)\n",
        "#end_date = date(2021, 6, 20)\n",
        "#delta = timedelta(days=1)\n",
        "start_date = date.today() - timedelta(2)\n",
        "end_date = start_date\n",
        "delta = timedelta(days=1)\n",
        "\n",
        " \n",
        "\n",
        "start_date_para = str(start_date)\n",
        "end_date_para = str(end_date)\n",
        "\n",
        " \n",
        "\n",
        "inventory_data_source_objects = ''\n",
        "date_objects = ''\n",
        "while start_date <= end_date:\n",
        "    try:\n",
        "        datenow = start_date.strftime(\"%Y%m%d\")\n",
        "        year = datenow[:4]\n",
        "        month = datenow[4:6]\n",
        "        day = datenow[6:8]\n",
        "        trans_date = start_date.strftime(\"%Y-%m-%d\")\n",
        "        inventory_data_out_path = \"inventory_temp_out/thales/inventory/dt=\" + year + month + day + \"/part-*\"\n",
        "        inventory_data_source_objects += inventory_data_out_path\n",
        "        date_objects += \"'\" + str(start_date) + \"',\"\n",
        "        start_date += delta\n",
        "    except:\n",
        "        start_date += delta\n",
        "inventory_data_source_objects = inventory_data_source_objects.replace(\"part-*\", \"part-*,\")\n",
        "inventory_data_source_objects = inventory_data_source_objects[:-1]\n",
        "date_objects = date_objects[:-1]\n",
        "date_objects = \"(\" + date_objects + \")\"\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "def Convert(string):\n",
        "    li = list(string.split(\",\"))\n",
        "    return li\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "inventory_data_source_objects_final = Convert(inventory_data_source_objects)\n",
        "\n",
        " \n",
        "\n",
        "# STEP 3: Set default arguments for the DAG\n",
        "DEFAULT_DAG_ARGS = {\n",
        "    'owner': 'eSIM Developer',  # The owner of the task.\n",
        "    # Task instance should not rely on the previous task's schedule to succeed.\n",
        "    'depends_on_past': False,\n",
        "    # We use this in combination with schedule_interval=None to only trigger the DAG with a\n",
        "    # POST to the REST API.\n",
        "    # Alternatively, we could set this to yesterday and the dag will be triggered upon upload to the\n",
        "    # dag folder.\n",
        "    'start_date': '2021-06-22',\n",
        "    # 'email': None,\n",
        "    # 'email_on_failure': True,\n",
        "    # 'email_on_retry': True,\n",
        "    'retries': 1,  # Retry once before failing the task.\n",
        "    'retry_delay': datetime.timedelta(minutes=1),  # Time between retries.\n",
        "    # 'project_id': Variable.get('gcp_project'),  # Cloud Composer project ID.\n",
        "    'project_id': 'vf-grp-cpsa-prd-cpsoi-10',  # Cloud Composer project ID.\n",
        "    # We only want the DAG to run when we POST to the api.\n",
        "    # Alternatively, this could be set to '@daily' to run the job once a day.\n",
        "    # more options at https://airflow.apache.org/scheduler.html#dag-runs\n",
        "}\n",
        "\n",
        " \n",
        "\n",
        "# STEP 4: Define DAG\n",
        "# set the DAG name, add a DAG description, define the schedule interval and pass the default arguments defined before\n",
        "with DAG('eSIM-thales-inventory-service', description='DAG for deployment a Dataproc Cluster',\n",
        "         default_args=DEFAULT_DAG_ARGS,\n",
        "         schedule_interval='30 4 * * *') as dag:  # Here we are using dag as context.'30 3 * * *'\n",
        "    # STEP 5: Set Operators\n",
        "    # Create the Cloud Dataproc cluster.\n",
        "    # Note: this operator will be flagged a success if the cluster by this name already exists.\n",
        "    create_cluster = DataprocClusterCreateOperator(\n",
        "        task_id='create_dataproc_cluster',\n",
        "        # ds_nodash is an airflow macro for \"[Execution] Date string no dashes\"\n",
        "        # in YYYYMMDD format. See docs https://airflow.apache.org/code.html?highlight=macros#macros\n",
        "        cluster_name='esim-thales-inv-dp-cluster-{{ ds_nodash }}',\n",
        "        num_workers=4,\n",
        "        master_machine_type='n1-standard-8',\n",
        "        master_disk_size=1000,\n",
        "        worker_machine_type='n1-standard-16',\n",
        "        worker_disk_size=1000,\n",
        "        zone='europe-west1-b',\n",
        "        ##network_uri='vodafone-cpsoi-dev-vpc',\n",
        "        subnetwork_uri='vodafone-vf-grp-cpsa-prd-cpsoi-10-prd-be-net',\n",
        "        region='europe-west1',\n",
        "        tags='allow-ssh',\n",
        "        project_id='vf-grp-cpsa-prd-cpsoi-10')\n",
        "\n",
        " \n",
        "\n",
        "    # Submit the PySpark job.\n",
        "    # For GND inventory\n",
        "    submit_pyspark_thales_inventory = DataProcPySparkOperator(\n",
        "        task_id='thales_inventory_run_dataproc_pyspark',\n",
        "        main=spark_job_esim_thales_inventory,\n",
        "        # Obviously needs to match the name of cluster created in the prior Operator.\n",
        "        cluster_name='esim-thales-inv-dp-cluster-{{ ds_nodash }}',\n",
        "        region='europe-west1',\n",
        "        arguments=[\n",
        "            'gs://vf-grp-cpsa-prd-cpsoi-10-esim-output/services/esim/thales/inventory',\n",
        "            'gs://' + BUCKET + '/inventory_temp_out',\n",
        "            start_date_para,\n",
        "            end_date_para\n",
        "        ]\n",
        "    )\n",
        "\n",
        " \n",
        "\n",
        "    # Delete the Cloud Dataproc cluster.\n",
        "    delete_cluster = DataprocClusterDeleteOperator(\n",
        "        task_id='delete_dataproc_cluster',\n",
        "        # Obviously needs to match the name of cluster created in the prior two Operators.\n",
        "        cluster_name='esim-thales-inv-dp-cluster-{{ ds_nodash }}',\n",
        "        region='europe-west1',\n",
        "        project_id='vf-grp-cpsa-prd-cpsoi-10',\n",
        "        # This will tear down the cluster even if there are failures in upstream tasks.\n",
        "        trigger_rule=TriggerRule.ALL_DONE)\n",
        "\n",
        " \n",
        "\n",
        "    # Delete the existing partionDATA from BQ table if exists\n",
        "    bq_delete_existing_partitionData = bigquery_operator.BigQueryOperator(\n",
        "        task_id='Deleting_existing_partitionData',\n",
        "        bql=\"\"\" DELETE FROM `{thales_inventory}` WHERE dt in {max_date}\n",
        "            \"\"\".format(max_date=date_objects, thales_inventory=output_table_esim_thales_inventory),\n",
        "        use_legacy_sql=False,\n",
        "        bigquery_conn_id='bigquery_default',\n",
        "        # destination_dataset_table=False,\n",
        "        trigger_rule=TriggerRule.ALL_DONE)\n",
        "\n",
        " \n",
        "\n",
        "    # Load the transformed files to a BigQuery table.\n",
        "    bq_load_thales_inventory = GoogleCloudStorageToBigQueryOperator(\n",
        "        task_id='Loading_GCS_to_BigQuery_thales_inventory',\n",
        "        bucket=BUCKET,\n",
        "        # Reads the relative path to the objects transformed by the spark job.\n",
        "        source_objects=inventory_data_source_objects_final,\n",
        "        destination_project_dataset_table=output_table_esim_thales_inventory,\n",
        "        schema_fields=None,\n",
        "        schema_object=None,\n",
        "        autodetect=True,\n",
        "        source_format='Parquet',\n",
        "        time_partitioning={'field': 'dt', 'type': 'DAY'},\n",
        "        create_disposition='CREATE_IF_NEEDED',\n",
        "        skip_leading_rows=0,\n",
        "        write_disposition='WRITE_APPEND',\n",
        "        max_bad_records=0,\n",
        "        region='europe-west1',\n",
        "        project_id='vf-grp-cpsa-prd-cpsoi-10',\n",
        "        trigger_rule=TriggerRule.ALL_DONE)\n",
        "\n",
        " \n",
        "\n",
        "    # Delete  gcs files in the timestamped transformed folder.\n",
        "    delete_transformed_files_thales_inventory = GoogleCloudStorageDeleteOperator(\n",
        "        task_id='delete_transformed_files_thales_inventory',\n",
        "        bucket_name=BUCKET,\n",
        "        prefix='inventory_temp',\n",
        "        objects=None,\n",
        "        region='europe-west1',\n",
        "        project_id='vf-grp-cpsa-prd-cpsoi-10',\n",
        "        trigger_rule=TriggerRule.ALL_DONE)\n",
        "\n",
        " \n",
        "\n",
        "# STEP 6: Order of execution of task that defined.\n",
        "create_cluster >> submit_pyspark_thales_inventory >> delete_cluster >> bq_delete_existing_partitionData >> bq_load_thales_inventory >> delete_transformed_files_thales_inventory\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}